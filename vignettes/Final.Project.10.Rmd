---
title: "`Final.Project.10`  Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Final.Project.10}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Source: <https://github.com/AU-R-Data-Science/Final.Project.10>

GitHub repository: `Final.Project.10`

Package: `Final.Project.10`

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE
  # , comment = ">#"
)
```

```{r prepare, include=FALSE}
library(devtools)
install_github("https://github.com/AU-R-Data-Science/Final.Project.10")
```

## Introduction

The `Final.Project.10` package in based on the final project assignment of STAT-6210. This document explains and gives examples of how to use the package functions for all the desired outputs using Weekly and mtcars dataset.

And the develop **shiny app** stored in <https://github.com/AU-R-Data-Science/Final.Project.10/tree/main/Shiny%20app>, please check it separately.

```{r setup}
library(Final.Project.10)
```

The available functions allow the user to input data sets to fit a logistic regression. The output includes: 1. estimated logistic regression model parameters; 2. initial values of the parameters; 3. confidence intervals for each parameter; 4. fitted logistic regression curves; 5. confusion matrix; 6. six metrics based on default cut-off values (Prevalence, Accuracy, Sensitivity, Specificity, False Discovery Rate and Diagnostic Odds Ratio) and 7. metrics with different cut-off values.

This document shows the usage and output of the functions in `Final.Project.10`, using the `Weekly` dataset from the `ISLR` package and the `mtcars` dataset as examples. The `Weekly` dataset described the weekly percentage returns for the S&P 500 stock index between 1990 and 2010. And the `mtcars` was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973--74 models).

## Data Pre-Processing

As an example, here we utilize the dataset `Weekly` from package `ISLR`

```{r}
library(ISLR)
str(Weekly)
```

The `Weekly` dataset has 1089 observations which contains 9 attributes, where `Lag1`, `Lag2`, `Lag3`, `Lag4`, `Lag5` and `Volume`, can be used as predictor, and `Direction` can be used as response. The `mtcars` has 32 observations which contains 11 attributes, where `mpg`, `dis` and `hp` can be used as predictor, and `vs` can be used as response.

It is important to note that the response values referenced in the functions available in this package need to be in factor format. Therefore, the `vs` variable in `mtcars` should be converted to factor format before calling the function.

We want to take only the parameters `Lag1`, `Lag2`, `Lag3`, `Lag4`, `Lag5` and `Volume` as input.

```{r}
Training.set.1 <- Weekly
head(Training.set.1)
```

```{r}
X_train <- Training.set.1[,c("Lag1","Lag2","Lag3","Lag4","Lag5","Volume")]
y_train <- Training.set.1$Direction
```

## Summary function `logistic.regression()`

The `logistic.regression()` contains all the functions that the package is can implement, it is a summary of the analysis, and its output is a list data that include the following, and these values can also be calculated by separate functions. The help document of all these functions can be found in the package.

1.  The level of response;

2.  Estimated \Beta value; (`Beta.hat()`)

3.  Initial \Beta value; (`Beta.init()`)

4.  The Confidence interval of \Beta; (`boot.confi()`)

5.  Confusion Matrix; (`boot.confi()`)

6.  Default metrics(with 0.5 cut-off value); (`confusion.matrix()`)

7.  Table of different cut-off and metrics; (`metrics.table()`)

8.  Plot of different cut-off and metrics;

9.  Logistic curve for each perdictor. (`logistic_plot()`)

## Train the model

Use the function `logistic.regression()` to fit the model. Note that the parameter `Y` must be factors.

```{r}
fit <- logistic.regression(X_train, y_train, # required
                           cutoff = 0.5,     # If you don't specify, the default will be 0.5
                           alpha = 0.1,      # If you don't specify, the default will be 0.1
                           B = 20)            # If you don't specify, the default will be 20
```

The user is able to choose the significance level \alpha to obtain for the 1âˆ’\alpha confidence intervals for \beta, as well as the cutoff value, where the default value is set to *0.1* and *0.5*, respectively.

The user is also able to choose the number of bootstraps which by default will be *20*.

## Chech the optimization parameters

Initial values for optimization obtained from the least-squares:

```{r}
optimization <- fit$Beta
knitr::kable(optimization)
```

The optimal \Beta is the same when we use the embeded `glm` method of R:

```{r}
fit.glm <- glm(data = Weekly, Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, family = binomial)
fit.glm$coefficients
```

## Bootstrap confidence Interval

The bootstrap process was performed when we called `logistic.regression()`. And the confidence interval can also be calculated by `boot.confi()`.

```{r}
CI <- boot.confi(X=X_train,Y=y_train,alpha=0.05,B=20)
knitr::kable(CI)
```

## Predict and Inspect the output

From that we obtain the resulting confusion matrix using a cut-off value for prediction at 0.5:

```{r}
confusion_Matrix <- fit$Confusion.Matrix
knitr::kable(confusion_Matrix)
```

And all the statistical metrics, including `Prevalence`, `Accuracy`, `Sensitivity`, `Specificity`, `False Discovery Rate`, and `Diagnostic Odds Ratio`, can be output by:

```{r}
m <- fit$Metrics
knitr::kable(m)
```

Too see how the metrics vary when we use different cut-off values, check the `Metrics Table`:

```{r}
metrics.table <- fit$'Metrics Table'
knitr::kable(metrics.table)
```

We can see that some of the value in this table became NaN or Inf, that is because when the cut-off value is too small or too high, the model may classify all the test data to be positive or all negative, which will make the denominator to be 0 in the calculation.

## Visualization of output:

The possibility for the user to plot of any of the above metrics evaluated over a grid of cut-off values for prediction going from 0.1 to 0.9 with steps of 0.1. (The graph ignores the missing Diagnostic odds ratio and false discovery rate values.)

```{r,  fig.height=3.5, fig.width=6}
fit$`Cut-off image`
```

## Fitted Logistic curve

To demonstrate the function `logistic_plot()` for plotting logistic curves in packages, this section uses `mtcars` as the dataset, where `mpg`, `dis` and `hp` are used as predictors and `vs` is used as the response. The output of the `logistic_plot()` function is in list format, which contains the logistic regression curves for each predictor. For example, in this example the first element represents the logistic regression curve between `mpg` and `vs`, the second element represents the logistic regression curve between `disp` and `vs`, and the third element represents the logistic regression curve between `hp` and `vs`.

```{r, results='hold'}
Training.set.2 <- mtcars
head(Training.set.2)
Training.set.2$vs <- as.factor(Training.set.2$vs)
logistic_plot(X=Training.set.2[,c("mpg","disp","hp")],Y=Training.set.2[,"vs"])
```






## Categorical Variables data processing

The demonstration above showed how to use our model to classify data with numeric variables.
For data with categorical variables, the user need to perform specific data preprocessing to their categorical features.
Here we use the dataset `"adult.csv"` from Canvas course page as an example.
```{r, results = 'hold'}
# Fetch and inspect dataset
Training.set.3 <- read.csv("archive/adult.csv", sep = ";")
str(Training.set.3)
nrow(Training.set.3)
```

Assume that we want to use the numeric variables `age` and `hours.per.week`, and categoric variables `sex` and `race`.
```{r, results='hold'}
# For demonstration purpose, we only run on 1000 random samples
sample_index = sample(nrow(Training.set.3), 1000, replace = FALSE)

X_train_3 <- Training.set.3[sample_index,c('age','hours.per.week','sex','race')]
y_train_3 <- Training.set.3[sample_index,16]

# Inspect 5 rows training data
head(X_train_3, 5)
print("Sex includes:")
levels(as.factor(X_train_3$sex))    # Check how many sex categories are there
print("Race includes:")
levels(as.factor(X_train_3$race))   # Check how many sex categories are there
```

For categorical variables user need to cast them into numeric variabls by casting the column into binary values.
```{r}
# Cast sex category to binary
X_train_3$sex <- ifelse(X_train_3$sex == " Male", 1, 0)
names(X_train_3)[3] = 'is.Male'
```

When the number of categories is larger than two, the user need to use `n-1` columns of binary values to represent these categories, where `n` represents the number of categories.
```{r}
# Cast race category to binary
X_train_3$is.Black <- ifelse(X_train_3$race == " Black", 1, 0)
X_train_3$is.AsianPac <- ifelse(X_train_3$race == " Amer-Indian-Eskimo", 1, 0)
X_train_3$is.AmerInd <- ifelse(X_train_3$race == " Asian-Pac-Islander", 1, 0)
X_train_3$race <- ifelse(X_train_3$race == " White", 1, 0)
names(X_train_3)[4] = "is.White"

# There is a fifth race category, " Other", but it will be a linearly dependent column 
#     to the other four, i.e. strongly correlated variable, which will fail R's embedded 
#     optimization function. So, we don't need it.


# Inspect training data (X)
head(X_train_3, 5)

# Cast label (y) to binary factor
y_train_3 <- ifelse(y_train_3 == " >50K", 1, 0)
y_train_3 <- as.factor(y_train_3)
```


```{r}
fit_3 <- logistic.regression(X_train_3, y_train_3, # required
                           cutoff = 0.5,     # If you don't specify, the default will be 0.5
                           alpha = 0.1,      # If you don't specify, the default will be 0.1
                           B = 20)      
```



And the corresponding result will be:
```{r}
optimization_3 <- fit_3$Beta
knitr::kable(optimization_3)

fit_3$Metrics

confusion_Matrix_3 <- fit_3$Confusion.Matrix
knitr::kable(confusion_Matrix_3)

metrics.table_3 <- fit_3$'Metrics Table'
knitr::kable(metrics.table_3)
```
